{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b46ac6",
   "metadata": {},
   "source": [
    " üåê Kernel PCA in Python: Improving Classification Accuracy with Feature Extraction\n",
    "\n",
    "## üß† What is Kernel PCA?\n",
    "\n",
    "**Kernel PCA** is an extension of **Principal Component Analysis (PCA)** that uses **kernel functions** to perform **nonlinear dimensionality reduction**.\n",
    "\n",
    "> Just like how **Kernel SVM** improves **SVM** by handling nonlinear data, **Kernel PCA** improves regular PCA by projecting data into a **higher-dimensional space** using a kernel.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Goal of Kernel PCA\n",
    "\n",
    "* Reduce dimensionality while capturing **nonlinear relationships**.\n",
    "* Help models (like logistic regression) perform better by projecting data into a more separable space.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How Kernel PCA Works\n",
    "\n",
    "1. Maps data to a **higher-dimensional space** using a kernel.\n",
    "2. Performs PCA in that new space.\n",
    "3. Returns **principal components** capturing nonlinear structure.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Kernel PCA vs PCA vs LDA\n",
    "\n",
    "| Technique      | Type         | Focus                       | Uses Labels? | Handles Nonlinearity? |\n",
    "| -------------- | ------------ | --------------------------- | ------------ | --------------------- |\n",
    "| PCA            | Unsupervised | Maximize variance           | ‚ùå No         | ‚ùå No                  |\n",
    "| LDA            | Supervised   | Maximize class separation   | ‚úÖ Yes        | ‚ùå No                  |\n",
    "| **Kernel PCA** | Unsupervised | Capture nonlinear structure | ‚ùå No         | ‚úÖ Yes                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Dataset Description\n",
    "\n",
    "* **Dataset**: `wine.csv`\n",
    "* **Task**: Classify wine samples into 3 customer segments based on features like alcohol, magnesium, and proline.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Python Implementation of Kernel PCA\n",
    "\n",
    "```python\n",
    "# Step 1: Import Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Step 2: Apply Kernel PCA with RBF kernel\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "X_train_kpca = kpca.fit_transform(X_train)\n",
    "X_test_kpca = kpca.transform(X_test)\n",
    "\n",
    "# Step 3: Train a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_kpca, y_train)\n",
    "\n",
    "# Step 4: Predict and evaluate\n",
    "y_pred = classifier.predict(X_test_kpca)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "‚úÖ **Expected Result:** **100% Accuracy** (compared to lower accuracy with PCA alone)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Visualizing Kernel PCA Results\n",
    "\n",
    "* Plot the 2 components from `X_test_kpca`\n",
    "* The transformation via RBF kernel results in better **class separation**.\n",
    "* Logistic Regression performs **perfect classification** due to clearer decision boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Key Benefits of Kernel PCA\n",
    "\n",
    "* **Nonlinear dimensionality reduction**\n",
    "* **Improved classification accuracy**\n",
    "* **Better feature extraction** for models like SVM, Logistic Regression, etc.\n",
    "* **Transforms data into a space where classes are linearly separable**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Practice Suggestions\n",
    "\n",
    "* Try Kernel PCA on datasets from:\n",
    "\n",
    "  * [UCI Machine Learning Repository](https://archive.ics.uci.edu/)\n",
    "  * Kaggle datasets\n",
    "* Compare PCA vs Kernel PCA results:\n",
    "\n",
    "  * Accuracy\n",
    "  * Visual separation\n",
    "  * Confusion matrix\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Key Takeaways\n",
    "\n",
    "* **Kernel PCA** = PCA + Kernel Trick (e.g., RBF, poly).\n",
    "* Great for **nonlinear patterns** where PCA struggles.\n",
    "* Similar syntax to PCA in `scikit-learn` with just one key change: `kernel='rbf'`.\n",
    "* A powerful preprocessing tool before classification.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ What‚Äôs Next?\n",
    "\n",
    "* Learn **K-Fold Cross-Validation** to evaluate models.\n",
    "* Apply **Grid Search** for hyperparameter tuning.\n",
    "* Master **XGBoost** for superior classification performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
