{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f890cc",
   "metadata": {},
   "source": [
    "# üìâ PCA Algorithm Intuition: Reducing Dimensions in Unsupervised Learning\n",
    "\n",
    "## üìå What is PCA?\n",
    "\n",
    "PCA (Principal Component Analysis) is a powerful **unsupervised learning algorithm** used for:\n",
    "\n",
    "* üîπ **Dimensionality reduction**\n",
    "* üîπ **Data visualization**\n",
    "* üîπ **Noise filtering**\n",
    "* üîπ **Feature extraction**\n",
    "\n",
    "Instead of predicting outputs like regression, PCA **finds patterns in data** by identifying directions (axes) where the data **varies the most**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Reduce Dimensions?\n",
    "\n",
    "* ‚úÖ Reduces **computation time**\n",
    "* ‚úÖ Removes **redundant** or **correlated** features\n",
    "* ‚úÖ Helps in **visualizing** high-dimensional data (e.g., 100D ‚Üí 2D)\n",
    "* ‚úÖ Makes **machine learning models** faster and often better\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ How Does PCA Work? (5-Step Algorithm)\n",
    "\n",
    "### Step 1: **Standardize the Data**\n",
    "\n",
    "Make sure all features have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: **Compute Covariance Matrix**\n",
    "\n",
    "This matrix captures relationships (correlations) between features.\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X) = \\frac{1}{n-1} (X^T X)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: **Get Eigenvectors and Eigenvalues**\n",
    "\n",
    "* **Eigenvectors** ‚Üí directions (axes) of max variance (called *principal components*)\n",
    "* **Eigenvalues** ‚Üí amount of variance captured by each direction\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: **Sort Eigenvectors by Eigenvalues**\n",
    "\n",
    "Keep only the **top K** eigenvectors that capture most of the variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: **Project Data onto New Subspace**\n",
    "\n",
    "Transform original data into the lower-dimensional space:\n",
    "\n",
    "$$\n",
    "X_{\\text{new}} = X \\cdot W\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $W$ = matrix of top K eigenvectors\n",
    "* $X_{\\text{new}}$ = reduced data\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Simple Intuition (with a Diagram)\n",
    "\n",
    "Imagine a **cloud of points** in 3D space that mostly spreads along one diagonal direction. PCA rotates the axes to align with this direction and **projects** the data onto this new axis.\n",
    "\n",
    "```\n",
    "Original axes: x, y, z\n",
    "New axes: PC1 (most variance), PC2, PC3 (least variance)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Easy Python Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample 2D data\n",
    "X = np.array([\n",
    "    [2.5, 2.4],\n",
    "    [0.5, 0.7],\n",
    "    [2.2, 2.9],\n",
    "    [1.9, 2.2],\n",
    "    [3.1, 3.0],\n",
    "    [2.3, 2.7],\n",
    "    [2, 1.6],\n",
    "    [1, 1.1],\n",
    "    [1.5, 1.6],\n",
    "    [1.1, 0.9]\n",
    "])\n",
    "\n",
    "# Step 1: Standardize\n",
    "X_meaned = X - np.mean(X, axis=0)\n",
    "\n",
    "# Step 2: Covariance matrix\n",
    "cov_matrix = np.cov(X_meaned, rowvar=False)\n",
    "\n",
    "# Step 3: Eigen decomposition\n",
    "eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Step 4: Sort eigenvectors\n",
    "sorted_index = np.argsort(eigen_values)[::-1]\n",
    "eigen_vectors = eigen_vectors[:, sorted_index]\n",
    "eigen_values = eigen_values[sorted_index]\n",
    "\n",
    "# Step 5: Project onto new basis (reduce to 1D)\n",
    "n_components = 1\n",
    "eigenvector_subset = eigen_vectors[:, 0:n_components]\n",
    "X_reduced = np.dot(X_meaned, eigenvector_subset)\n",
    "\n",
    "print(\"Reduced Data:\\n\", X_reduced)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "| Feature      | PCA                                       |\n",
    "| ------------ | ----------------------------------------- |\n",
    "| Type         | Unsupervised                              |\n",
    "| Goal         | Reduce dimensions, keep variance          |\n",
    "| Technique    | Eigen decomposition                       |\n",
    "| Use Cases    | Visualization, compression, preprocessing |\n",
    "| Sensitive to | **Outliers** (may distort directions)     |\n",
    "| Output       | Transformed lower-dimensional data        |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Limitation\n",
    "\n",
    "* PCA assumes **linear relationships**.\n",
    "* Sensitive to **scaling** and **outliers**.\n",
    "* Cannot capture **nonlinear patterns** (use t-SNE or UMAP for that).\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Visual Demo Tools\n",
    "\n",
    "Try these for hands-on visualization:\n",
    "\n",
    "* [PCA 2D & 3D visual tool](https://setosa.io/ev/principal-component-analysis/)\n",
    "* [Explained Visually (by Victor Powell)](https://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "* PCA finds **patterns** in data by learning new **axes of maximum variance**.\n",
    "* Reduces dimensionality while retaining the most **important information**.\n",
    "* Easy to implement using NumPy or libraries like `scikit-learn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d1aa9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
