{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd932de",
   "metadata": {},
   "source": [
    "# üéØ How to Use XGBoost in Python for Cancer Prediction with High Accuracy\n",
    "\n",
    "## üîç Introduction to XGBoost\n",
    "\n",
    "XGBoost (**eXtreme Gradient Boosting**) is a highly effective machine learning algorithm that works well for:\n",
    "\n",
    "* ‚úÖ Classification tasks\n",
    "* ‚úÖ Regression tasks\n",
    "\n",
    "It is fast, accurate, and regularly wins machine learning competitions due to its performance and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## ü©∫ Dataset Overview\n",
    "\n",
    "We'll use a breast cancer dataset with features like:\n",
    "\n",
    "* Clump thickness\n",
    "* Uniformity of cell size\n",
    "* Uniformity of cell shape\n",
    "  ... and others.\n",
    "\n",
    "### üéØ Target Variable:\n",
    "\n",
    "* **Benign tumor** ‚Üí class `2`\n",
    "* **Malignant tumor** ‚Üí class `4`\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Previous Model Performances\n",
    "\n",
    "| Model               | Accuracy                  |\n",
    "| ------------------- | ------------------------- |\n",
    "| Logistic Regression | 94.7%                     |\n",
    "| k-Nearest Neighbors | 94.7%                     |\n",
    "| SVM                 | 94.1%                     |\n",
    "| Kernel SVM          | 95.3%                     |\n",
    "| Naive Bayes         | 94.1%                     |\n",
    "| Decision Tree       | **95.9%** ‚úÖ (best so far) |\n",
    "| Random Forest       | 93.5%                     |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Environment Setup\n",
    "\n",
    "* Recommended: **Google Colab** or **Jupyter Notebook**\n",
    "* Dataset: `data.csv`\n",
    "\n",
    "> üí° In Google Colab, upload `data.csv` before running the code.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Import Libraries and Prepare the Data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('data.csv')\n",
    "\n",
    "# Split features and labels\n",
    "X = dataset.iloc[:, :-1].values  # all columns except last\n",
    "y = dataset.iloc[:, -1].values   # target variable\n",
    "\n",
    "# Encode labels if needed\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Training the XGBoost Classifier\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize and train the classifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Evaluating the Model\n",
    "\n",
    "### 1. Accuracy and Confusion Matrix\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "```\n",
    "\n",
    "### 2. ‚úÖ k-Fold Cross-Validation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "accuracies = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=10)\n",
    "print(\"Cross-Validation Accuracy: {:.2f}%\".format(accuracies.mean() * 100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std() * 100))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä XGBoost for Regression (Bonus)\n",
    "\n",
    "For regression tasks, use `XGBRegressor` instead:\n",
    "\n",
    "```python\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "regressor = XGBRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Final Results\n",
    "\n",
    "* **XGBoost Classifier Accuracy:** **97.8%**\n",
    "* **k-Fold Cross-Validation Avg Accuracy:** **96.53%**\n",
    "* **Standard Deviation:** \\~2%\n",
    "\n",
    "‚úÖ This **outperformed** all previously tested models!\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "XGBoost:\n",
    "\n",
    "* Is one of the most powerful tools for classification and regression\n",
    "* Delivered the **highest accuracy** on the cancer dataset\n",
    "* Proved its **robustness** through k-Fold Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Key Takeaways\n",
    "\n",
    "* üõ† **XGBoost** is an essential model in your ML toolbox.\n",
    "* üîç It works on both **classification and regression** problems.\n",
    "* üéØ Achieved **97.8% accuracy** in cancer detection.\n",
    "* üìä k-Fold validation confirmed its **robust performance**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a712e",
   "metadata": {},
   "source": [
    "## üß† Goal:\n",
    "\n",
    "Use **XGBoost** to **predict if a tumor is benign or malignant** with high accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Dataset Example:\n",
    "\n",
    "We use the **Breast Cancer Wisconsin dataset** where:\n",
    "\n",
    "| Feature                  | Example Value               |\n",
    "| ------------------------ | --------------------------- |\n",
    "| Clump Thickness          | 5                           |\n",
    "| Uniformity of Cell Size  | 1                           |\n",
    "| Uniformity of Cell Shape | 3                           |\n",
    "| ...                      | ...                         |\n",
    "| Class (Target)           | 2 (Benign) or 4 (Malignant) |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Step 1: Install and Import\n",
    "\n",
    "```bash\n",
    "pip install xgboost\n",
    "```\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Step 2: Load and Prepare the Data\n",
    "\n",
    "```python\n",
    "data = pd.read_csv('data.csv')\n",
    "X = data.drop(columns=['Class'])  # Features\n",
    "y = data['Class']                 # Target: 2 (Benign), 4 (Malignant)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Step 3: Build and Train XGBoost Classifier\n",
    "\n",
    "```python\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Step 4: Make Predictions and Evaluate\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Diagram: What XGBoost Does Internally\n",
    "\n",
    "Imagine this as an **ensemble of decision trees**:\n",
    "\n",
    "```\n",
    "XGBoost = Tree 1 + Tree 2 + Tree 3 + ... + Tree N\n",
    "\n",
    "Each tree corrects the previous tree's mistake!\n",
    "```\n",
    "\n",
    "### Example:\n",
    "\n",
    "* **Tree 1** says: 80% accuracy ‚Üí but some malignant tumors are misclassified.\n",
    "* **Tree 2** learns from Tree 1‚Äôs mistakes.\n",
    "* **Tree 3** improves further.\n",
    "* Final prediction: combines results of all trees ‚Üí more accurate!\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Bonus: k-Fold Cross-Validation\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=10)\n",
    "print(\"Mean Accuracy: {:.2f}%\".format(scores.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(scores.std()*100))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Final Output Example\n",
    "\n",
    "```\n",
    "Accuracy: 97.8%\n",
    "Confusion Matrix:\n",
    "[[91  0]\n",
    " [ 3 46]]\n",
    "Mean Accuracy: 96.53%\n",
    "Standard Deviation: 2.0%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Why XGBoost is So Powerful?\n",
    "\n",
    "‚úÖ Boosting: Fixes mistakes from previous trees\n",
    "‚úÖ Regularization: Avoids overfitting\n",
    "‚úÖ Handles missing values\n",
    "‚úÖ Fast and efficient\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Real-Life Analogy\n",
    "\n",
    "Think of XGBoost like a **panel of doctors**. One might misdiagnose, but together, their combined judgment is much more accurate.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary\n",
    "\n",
    "| Step | What We Did                             |\n",
    "| ---- | --------------------------------------- |\n",
    "| 1    | Imported libraries                      |\n",
    "| 2    | Loaded and prepared the data            |\n",
    "| 3    | Built and trained XGBoost model         |\n",
    "| 4    | Evaluated accuracy and confusion matrix |\n",
    "| 5    | Applied k-Fold CV for robustness        |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
