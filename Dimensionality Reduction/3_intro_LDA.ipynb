{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76084116",
   "metadata": {},
   "source": [
    "# üéØ LDA Intuition: Maximizing Class Separation in Machine Learning\n",
    "\n",
    "## üìå What is LDA?\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)** is a **supervised machine learning technique** used for:\n",
    "\n",
    "* **Dimensionality Reduction**\n",
    "* **Class Separation**\n",
    "* **Preprocessing for classification algorithms**\n",
    "\n",
    "> ‚ö†Ô∏è **Main Difference from PCA:**\n",
    ">\n",
    "> * PCA focuses on **maximizing variance** (unsupervised)\n",
    "> * LDA focuses on **maximizing class separability** (supervised)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Goal of LDA\n",
    "\n",
    "* Reduce data dimensions while **retaining the features that maximize class separation**.\n",
    "* Find new axes (called **linear discriminants**) that best separate the classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Visualization: PCA vs LDA\n",
    "\n",
    "| Technique | Type         | Objective                 | Uses Labels? |\n",
    "| --------- | ------------ | ------------------------- | ------------ |\n",
    "| PCA       | Unsupervised | Maximize variance         | ‚ùå No         |\n",
    "| LDA       | Supervised   | Maximize class separation | ‚úÖ Yes        |\n",
    "\n",
    "In 2D space:\n",
    "\n",
    "* **PCA** might choose the direction of overall spread.\n",
    "* **LDA** will choose the direction that best **separates** the classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 5-Step LDA Algorithm\n",
    "\n",
    "### Step 1: **Compute Class Mean Vectors**\n",
    "\n",
    "For each class $i$, compute the mean vector $\\mu_i$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: **Compute Scatter Matrices**\n",
    "\n",
    "* **Within-Class Scatter** $S_W$: Measures the spread **within each class**\n",
    "* **Between-Class Scatter** $S_B$: Measures the spread **between class means**\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: **Compute Eigenvectors and Eigenvalues**\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\n",
    "S_W^{-1} S_B \\vec{w} = \\lambda \\vec{w}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: **Sort Eigenvectors by Eigenvalues**\n",
    "\n",
    "Sort eigenvectors by descending eigenvalues (importance of separation).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: **Project Data**\n",
    "\n",
    "Use top $K$ eigenvectors to form projection matrix $W$, then:\n",
    "\n",
    "$$\n",
    "X_{\\text{new}} = X \\cdot W\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Python Example (with `scikit-learn`)\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = data.target_names\n",
    "\n",
    "for i, color, label in zip(range(3), colors, labels):\n",
    "    plt.scatter(X_lda[y == i, 0], X_lda[y == i, 1], alpha=0.7, label=label, color=color)\n",
    "\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.title('LDA: Iris Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† When to Use LDA?\n",
    "\n",
    "* When your data has **labels (supervised)**.\n",
    "* When you want to improve **classification accuracy**.\n",
    "* When the number of classes is **less than the number of features**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è LDA Limitations\n",
    "\n",
    "* Assumes **normal distribution** of features.\n",
    "* Assumes **equal covariance** across classes.\n",
    "* Works best with **linearly separable** classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Summary\n",
    "\n",
    "| Aspect        | PCA                        | LDA                          |\n",
    "| ------------- | -------------------------- | ---------------------------- |\n",
    "| Type          | Unsupervised               | Supervised                   |\n",
    "| Focus         | Maximize Variance          | Maximize Class Separation    |\n",
    "| Output Axes   | Principal Components       | Linear Discriminants         |\n",
    "| Needs Labels? | ‚ùå No                       | ‚úÖ Yes                        |\n",
    "| Usage         | Visualization, Compression | Classification Preprocessing |\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "* **LDA** is great for supervised dimensionality reduction.\n",
    "* It projects data in a way that **maximizes the separation** between multiple classes.\n",
    "* Helps classifiers like **Logistic Regression, SVM, etc.** perform better.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
